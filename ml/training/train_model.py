"""
ë³µì•½ ì§€ë„ ìƒì„± ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

í–¥í›„ LLaMA 3 ê¸°ë°˜ í•œêµ­ì–´ ëª¨ë¸ì„ Fine-tuningí•˜ëŠ” ì˜ˆì œ
í˜„ì¬ëŠ” ìŠ¤ì¼ˆë ˆí†¤ ì½”ë“œë§Œ ì œê³µ
"""
import json
import os
from typing import List, Dict

import torch
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
)

# TODO: ì‹¤ì œ í•™ìŠµ ì‹œ ì£¼ì„ í•´ì œ ë° ìˆ˜ì • í•„ìš”


class MedicationGuidanceTrainer:
    """ë³µì•½ ì§€ë„ ìƒì„± ëª¨ë¸ í•™ìŠµ í´ë˜ìŠ¤"""

    def __init__(self, model_name: str = "beomi/llama-3-open-ko-8b"):
        """
        ëª¨ë¸ ì´ˆê¸°í™”

        Args:
            model_name: Hugging Face ëª¨ë¸ ì´ë¦„
        """
        self.model_name = model_name
        print(f"ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")

        # TODO: ì‹¤ì œ í•™ìŠµ ì‹œ í™œì„±í™”
        # self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        # self.model = AutoModelForCausalLM.from_pretrained(
        #     model_name,
        #     torch_dtype=torch.float16,
        #     device_map="auto"
        # )

    def load_dataset(self, dataset_path: str) -> List[Dict]:
        """
        í•™ìŠµ ë°ì´í„°ì…‹ ë¡œë“œ

        Args:
            dataset_path: ë°ì´í„°ì…‹ JSON íŒŒì¼ ê²½ë¡œ

        Returns:
            ë°ì´í„°ì…‹ ë¦¬ìŠ¤íŠ¸
        """
        with open(dataset_path, "r", encoding="utf-8") as f:
            dataset = json.load(f)

        print(f"ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(dataset)}ê°œ")
        return dataset

    def prepare_training_data(self, dataset: List[Dict]):
        """
        í•™ìŠµ ë°ì´í„° ì „ì²˜ë¦¬

        Args:
            dataset: ì›ë³¸ ë°ì´í„°ì…‹

        Returns:
            ì „ì²˜ë¦¬ëœ ë°ì´í„°ì…‹
        """
        # TODO: í† í¬ë‚˜ì´ì§• ë° ì „ì²˜ë¦¬
        pass

    def train(self, train_dataset, eval_dataset=None):
        """
        ëª¨ë¸ í•™ìŠµ

        Args:
            train_dataset: í•™ìŠµ ë°ì´í„°ì…‹
            eval_dataset: ê²€ì¦ ë°ì´í„°ì…‹
        """
        # TODO: í•™ìŠµ ì„¤ì • ë° ì‹¤í–‰
        """
        training_args = TrainingArguments(
            output_dir="./models/medication-guidance-v1",
            num_train_epochs=3,
            per_device_train_batch_size=4,
            gradient_accumulation_steps=4,
            learning_rate=2e-5,
            fp16=True,
            logging_steps=10,
            save_steps=100,
            evaluation_strategy="steps" if eval_dataset else "no",
        )

        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
        )

        trainer.train()
        """
        print("âš ï¸  ëª¨ë¸ í•™ìŠµì€ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆ˜ì§‘ëœ í›„ ì§„í–‰í•©ë‹ˆë‹¤.")
        print("   í˜„ì¬ëŠ” ìŠ¤ì¼ˆë ˆí†¤ ì½”ë“œë§Œ ì œê³µë©ë‹ˆë‹¤.")

    def save_model(self, output_dir: str):
        """
        í•™ìŠµëœ ëª¨ë¸ ì €ì¥

        Args:
            output_dir: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        """
        # TODO: ëª¨ë¸ ì €ì¥
        """
        self.model.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)
        """
        pass


if __name__ == "__main__":
    print("=" * 60)
    print("ë©”ë””-ê³  ë³µì•½ ì§€ë„ ìƒì„± ëª¨ë¸ í•™ìŠµ")
    print("=" * 60)
    print()

    # ë°ì´í„°ì…‹ ê²½ë¡œ
    dataset_path = "../data/training_dataset.json"

    if not os.path.exists(dataset_path):
        print(f"âŒ ë°ì´í„°ì…‹ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {dataset_path}")
        print("   ë¨¼ì € prepare_dataset.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ì…‹ì„ ì¤€ë¹„í•˜ì„¸ìš”.")
        exit(1)

    print(f"ğŸ“ ë°ì´í„°ì…‹: {dataset_path}")
    print()

    print("ğŸš§ í˜„ì¬ëŠ” í•™ìŠµ ìŠ¤ì¼ˆë ˆí†¤ ì½”ë“œë§Œ ì œê³µë©ë‹ˆë‹¤.")
    print("   ì‹¤ì œ í•™ìŠµì€ ë‹¤ìŒ ì¡°ê±´ì´ ì¶©ì¡±ë˜ë©´ ì§„í–‰í•©ë‹ˆë‹¤:")
    print("   - ìµœì†Œ 500ê°œ ì´ìƒì˜ ê²€ì¦ëœ í•™ìŠµ ë°ì´í„°")
    print("   - GPU ì„œë²„ (AWS EC2 g4dn.xlarge ì´ìƒ)")
    print("   - ì¶©ë¶„í•œ ë©”ëª¨ë¦¬ (16GB+ VRAM)")
    print()
    print("ğŸ“Š í˜„ì¬ ë‹¨ê³„: MVP - ë°ì´í„° ìˆ˜ì§‘ ë‹¨ê³„")
    print("   ìš´ì˜íŒ€ì´ ìˆ˜ë™ìœ¼ë¡œ ë³µì•½ ì§€ë„ë¥¼ ì‘ì„±í•˜ë©´ì„œ ë°ì´í„°ë¥¼ ì¶•ì  ì¤‘ì…ë‹ˆë‹¤.")

